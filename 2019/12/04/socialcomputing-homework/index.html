<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="ZhangYufei's Blog" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="title: 对猫眼《海上钢琴师》影评进行情感分析date: 2019-12-04 16:01:17tags:  python 数据分析 深度学习   数据抓取因为猫眼没有严格的反爬机制，所以爬取数据还是比较顺利。选取了从电影上映（11月5日）到开始爬虫（11月26日）的影评，一共8000条左右。 代码没什么好说的。 123456789101112131415161718192021222324">
<meta property="og:type" content="article">
<meta property="og:title" content="socialcomputing-homework">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;12&#x2F;04&#x2F;socialcomputing-homework&#x2F;index.html">
<meta property="og:site_name" content="ZhangYufei&#39;s Blog">
<meta property="og:description" content="title: 对猫眼《海上钢琴师》影评进行情感分析date: 2019-12-04 16:01:17tags:  python 数据分析 深度学习   数据抓取因为猫眼没有严格的反爬机制，所以爬取数据还是比较顺利。选取了从电影上映（11月5日）到开始爬虫（11月26日）的影评，一共8000条左右。 代码没什么好说的。 123456789101112131415161718192021222324">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;12&#x2F;images&#x2F;snownlp.PNG">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;12&#x2F;images&#x2F;CNN.PNG">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;12&#x2F;images&#x2F;result.jpg">
<meta property="og:updated_time" content="2019-12-04T12:50:17.464Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;12&#x2F;images&#x2F;snownlp.PNG">

<link rel="canonical" href="http://yoursite.com/2019/12/04/socialcomputing-homework/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>socialcomputing-homework | ZhangYufei's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">ZhangYufei's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">爱你所爱，求你所求</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/04/socialcomputing-homework/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhang Yufei">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZhangYufei's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          socialcomputing-homework
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2019-12-04 16:01:17 / 修改时间：20:50:17" itemprop="dateCreated datePublished" datetime="2019-12-04T16:01:17+08:00">2019-12-04</time>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>14k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>12 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <hr>
<p>title: 对猫眼《海上钢琴师》影评进行情感分析<br>date: 2019-12-04 16:01:17<br>tags:</p>
<ul>
<li>python</li>
<li>数据分析</li>
<li>深度学习</li>
</ul>
<hr>
<h2 id="数据抓取"><a href="#数据抓取" class="headerlink" title="数据抓取"></a>数据抓取</h2><p>因为猫眼没有严格的反爬机制，所以爬取数据还是比较顺利。选取了从电影上映（11月5日）到开始爬虫（11月26日）的影评，一共8000条左右。</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>没什么好说的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="comment">#reload(sys)</span></span><br><span class="line"><span class="comment">#sys.setdefaultencoding("utf-8")</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># from fake_useragent import UserAgent</span></span><br><span class="line"></span><br><span class="line">USER_AGENTS = [<span class="string">"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)"</span>, <span class="string">"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)"</span>, <span class="string">"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)"</span>, <span class="string">"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)"</span>, <span class="string">"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)"</span>, <span class="string">"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)"</span>, <span class="string">"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)"</span>,</span><br><span class="line">               <span class="string">"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)"</span>, <span class="string">"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6"</span>, <span class="string">"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1"</span>, <span class="string">"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0"</span>, <span class="string">"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5"</span>, <span class="string">"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6"</span>, <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11"</span>, <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20"</span>, <span class="string">"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52"</span>, ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spidermaoyan</span><span class="params">()</span>:</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"User-Agent"</span>: random.choice(USER_AGENTS),</span><br><span class="line">        <span class="string">"Host"</span>: <span class="string">"m.maoyan.com"</span>,</span><br><span class="line">                <span class="string">"Referer"</span>: <span class="string">"http://m.maoyan.com/movie/1292/comments?_v_=yes"</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, url)</span>:</span></span><br><span class="line">        self.url = url</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 发送get请求</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_json</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 发送get请求</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            response_comment = requests.get(self.url, headers=self.headers)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> err:</span><br><span class="line">            print(err)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        json_comment = response_comment.text</span><br><span class="line">        json_comment = json.loads(json_comment)</span><br><span class="line">        <span class="comment"># print(json_comment)</span></span><br><span class="line">        <span class="keyword">return</span> json_comment</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取数据并存储</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(self, json_comment)</span>:</span></span><br><span class="line">        json_response = json_comment[<span class="string">"cmts"</span>]  <span class="comment"># 列表</span></span><br><span class="line">        <span class="comment"># print(json_response)</span></span><br><span class="line">        print(len(json_response))</span><br><span class="line">        list_info = []</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> json_response:</span><br><span class="line">            cityName = data.get(<span class="string">"cityName"</span>)</span><br><span class="line">            content = data.get(<span class="string">"content"</span>).replace(<span class="string">"\r\n"</span>,<span class="string">""</span>).replace(<span class="string">"\n"</span>,<span class="string">""</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="string">"gender"</span> <span class="keyword">in</span> data:</span><br><span class="line">                gender = data.get(<span class="string">"gender"</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                gender = <span class="number">0</span></span><br><span class="line">            nickName = data.get(<span class="string">"nickName"</span>)</span><br><span class="line">            userLevel = data.get(<span class="string">"userLevel"</span>)</span><br><span class="line">            score = data.get(<span class="string">"score"</span>)</span><br><span class="line">            time = data.get(<span class="string">"startTime"</span>)</span><br><span class="line">            date = data.get(<span class="string">"startTime"</span>).split()[<span class="number">0</span>]</span><br><span class="line">            list_one = [date, time, nickName, gender,</span><br><span class="line">                        cityName, userLevel, score, content]</span><br><span class="line">            list_info.append(list_one)</span><br><span class="line">        <span class="comment"># print(list_info)</span></span><br><span class="line">        self.file_do(list_info)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存储文件</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">file_do</span><span class="params">(self, list_info)</span>:</span></span><br><span class="line">        <span class="comment"># 获取文件大小</span></span><br><span class="line">        file_size = os.path.getsize(<span class="string">r'C:\Users\think\Desktop\情感分析\doc\maoyan.csv'</span>)</span><br><span class="line">        <span class="keyword">if</span> file_size == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 表头</span></span><br><span class="line">            name = [<span class="string">'date'</span>,<span class="string">'time'</span>,<span class="string">'name'</span>,<span class="string">'gender'</span>,<span class="string">'city'</span>,<span class="string">'userLevel'</span>,<span class="string">'score'</span>,<span class="string">'content'</span>]</span><br><span class="line">            <span class="comment"># 建立DataFrame对象</span></span><br><span class="line">            file_test = pd.DataFrame(columns=name, data=list_info)</span><br><span class="line">            <span class="comment"># 数据写入</span></span><br><span class="line">            file_test.to_csv(<span class="string">r'C:\Users\think\Desktop\情感分析\doc\maoyan.csv'</span>,</span><br><span class="line">                             encoding=<span class="string">'utf_8_sig'</span>, index=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">with</span> open(<span class="string">r'C:\Users\think\Desktop\情感分析\doc\maoyan.csv'</span>, <span class="string">'a+'</span>,</span><br><span class="line">                             encoding=<span class="string">'utf_8_sig'</span>) <span class="keyword">as</span> file_test:</span><br><span class="line">                <span class="comment"># 追加到文件后面</span></span><br><span class="line">                writer = csv.writer(file_test)</span><br><span class="line">                <span class="comment"># 写入文件</span></span><br><span class="line">                writer.writerows(list_info)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spider_maoyan</span><span class="params">()</span>:</span></span><br><span class="line">    start_time = datetime.datetime.now().strftime(<span class="string">'%Y-%m-%d %H:%M:%S'</span>)</span><br><span class="line">    end_time = <span class="string">'2019-11-15 00:00:00'</span></span><br><span class="line">    offset = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> start_time &gt; end_time:</span><br><span class="line">        comment_api = <span class="string">'http://m.maoyan.com/mmdb/comments/movie/1292.json?_v_=yes&amp;offset=15&amp;startTime='</span> + start_time.replace(<span class="string">' '</span>, <span class="string">'%20'</span>)</span><br><span class="line">        <span class="comment"># comment_api = 'http://m.maoyan.com/mmdb/comments/movie/1211270.json?_v_=yes&amp;offset=&#123;0&#125;&amp;startTime=&#123;1&#125;%2021%3A09%3A31'.format(offset, startTime)</span></span><br><span class="line">        s0 = Spidermaoyan(comment_api)</span><br><span class="line">        </span><br><span class="line">        json_comment = s0.get_json()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> json_comment:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        data = s0.get_data(json_comment)</span><br><span class="line">        start_time = data.get(<span class="string">"startTime"</span>)</span><br><span class="line">        start_time = datetime.datetime.strptime(start_time, <span class="string">'%Y-%m-%d %H:%M:%S'</span>) + datetime.timedelta(seconds=<span class="number">-1</span>)</span><br><span class="line">        start_time = datetime.datetime.strftime(start_time, <span class="string">'%Y-%m-%d %H:%M:%S'</span>)</span><br><span class="line">        offset = offset + <span class="number">15</span></span><br><span class="line">        time.sleep(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    spider_maoyan()</span><br></pre></td></tr></table></figure>


<h2 id="情感数据分析"><a href="#情感数据分析" class="headerlink" title="情感数据分析"></a>情感数据分析</h2><h3 id="使用SnowNLP库"><a href="#使用SnowNLP库" class="headerlink" title="使用SnowNLP库"></a>使用SnowNLP库</h3><p> SnowNLP是python中用来处理文本内容的，可以用来分词、标注、文本情感分析等，情感分析是简单的将文本分为两类，积极和消极，返回值为情绪的概率，越接近1为积极，接近0为消极。因为SnowNLP主要使用淘宝评论语料训练做情感挖掘，所以对影评的判断准确度可能不是特别高。<br> <a href="https://github.com/isnowfy/snownlp" target="_blank" rel="noopener">SnowNLP GitHub地址</a></p>
<h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Tue Dec  3 18:37:48 2019</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: think</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#import snownlp</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">from</span> snownlp <span class="keyword">import</span> SnowNLP</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">text = codecs.open(<span class="string">r'C:\Users\think\Desktop\情感分析\doc\demo.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>).read()</span><br><span class="line"><span class="comment">#zinput = str(input("Enter the word you want me to search: "))</span></span><br><span class="line">list = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">r'C:\Users\think\Desktop\情感分析\doc\answer.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> file:    </span><br><span class="line">        list.append(line)</span><br><span class="line"><span class="comment">#print(list)</span></span><br><span class="line"><span class="comment">#f = open(r'C:\Users\think\Desktop\情感分析\doc\answer.txt', 'r')</span></span><br><span class="line"><span class="comment">#list = f.readlines()</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file_do</span><span class="params">(list_info, flag)</span>:</span></span><br><span class="line">        <span class="comment"># 获取文件大小</span></span><br><span class="line">        <span class="comment"># file = 'C:\Users\think\Desktop\情感分析\doc\a\\'[:-1] + list_info + '.csv'</span></span><br><span class="line">        <span class="keyword">if</span> flag == <span class="number">0</span>:</span><br><span class="line">            file_size = os.path.getsize(<span class="string">r'C:\Users\think\Desktop\情感分析\doc\all.csv'</span>)</span><br><span class="line">            <span class="keyword">if</span> file_size == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 表头</span></span><br><span class="line">                name = [<span class="string">'content'</span>,<span class="string">'score'</span>]</span><br><span class="line">            <span class="comment"># 建立DataFrame对象</span></span><br><span class="line">                file_test = pd.DataFrame(columns=name, data=list_info)</span><br><span class="line">            <span class="comment"># 数据写入</span></span><br><span class="line">                file_test.to_csv(<span class="string">r'C:\Users\think\Desktop\情感分析\doc\all.csv'</span>,</span><br><span class="line">                             encoding=<span class="string">'utf_8_sig'</span>, index=<span class="literal">False</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">with</span> open(<span class="string">r'C:\Users\think\Desktop\情感分析\doc\all.csv'</span>, <span class="string">'a+'</span>,</span><br><span class="line">                             encoding=<span class="string">'utf_8_sig'</span>) <span class="keyword">as</span> file_test:</span><br><span class="line">                <span class="comment"># 追加到文件后面</span></span><br><span class="line">                    writer = csv.writer(file_test)</span><br><span class="line">                <span class="comment"># 写入文件</span></span><br><span class="line">                    writer.writerows(list_info)</span><br><span class="line">                    </span><br><span class="line">                    </span><br><span class="line">        <span class="keyword">if</span> flag == <span class="number">1</span>:</span><br><span class="line">            file_size = os.path.getsize(<span class="string">r'C:\Users\think\Desktop\情感分析\doc\neg.csv'</span>)</span><br><span class="line">            <span class="keyword">if</span> file_size == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 表头</span></span><br><span class="line">                name = [<span class="string">'content'</span>,<span class="string">'score'</span>]</span><br><span class="line">            <span class="comment"># 建立DataFrame对象</span></span><br><span class="line">                file_test = pd.DataFrame(columns=name, data=list_info)</span><br><span class="line">            <span class="comment"># 数据写入</span></span><br><span class="line">                file_test.to_csv(<span class="string">r'C:\Users\think\Desktop\情感分析\doc\neg.csv'</span>,</span><br><span class="line">                             encoding=<span class="string">'utf_8_sig'</span>, index=<span class="literal">False</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">with</span> open(<span class="string">r'C:\Users\think\Desktop\情感分析\doc\neg.csv'</span>, <span class="string">'a+'</span>,</span><br><span class="line">                             encoding=<span class="string">'utf_8_sig'</span>) <span class="keyword">as</span> file_test:</span><br><span class="line">                <span class="comment"># 追加到文件后面</span></span><br><span class="line">                    writer = csv.writer(file_test)</span><br><span class="line">                <span class="comment"># 写入文件</span></span><br><span class="line">                    writer.writerows(list_info)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> flag == <span class="number">2</span>:</span><br><span class="line">            file_size = os.path.getsize(<span class="string">r'C:\Users\think\Desktop\情感分析\doc\pos.csv'</span>)</span><br><span class="line">            <span class="keyword">if</span> file_size == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 表头</span></span><br><span class="line">                name = [<span class="string">'content'</span>,<span class="string">'score'</span>]</span><br><span class="line">            <span class="comment"># 建立DataFrame对象</span></span><br><span class="line">                file_test = pd.DataFrame(columns=name, data=list_info)</span><br><span class="line">            <span class="comment"># 数据写入</span></span><br><span class="line">                file_test.to_csv(<span class="string">r'C:\Users\think\Desktop\情感分析\doc\pos.csv'</span>,</span><br><span class="line">                             encoding=<span class="string">'utf_8_sig'</span>, index=<span class="literal">False</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">with</span> open(<span class="string">r'C:\Users\think\Desktop\情感分析\doc\pos.csv'</span>, <span class="string">'a+'</span>,</span><br><span class="line">                             encoding=<span class="string">'utf_8_sig'</span>) <span class="keyword">as</span> file_test:</span><br><span class="line">                <span class="comment"># 追加到文件后面</span></span><br><span class="line">                    writer = csv.writer(file_test)</span><br><span class="line">                <span class="comment"># 写入文件</span></span><br><span class="line">                    writer.writerows(list_info)</span><br><span class="line">sentimentslist = []</span><br><span class="line">list_all = []</span><br><span class="line">list_neg = []</span><br><span class="line">list_pos = []</span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> list:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> i <span class="keyword">is</span> <span class="string">'\n'</span>:</span><br><span class="line">        s = SnowNLP(i)</span><br><span class="line">        <span class="comment">#print(i)</span></span><br><span class="line">        <span class="comment">#print(s.sentiments)</span></span><br><span class="line">        print(count)</span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line">        <span class="comment">#if count == 10:</span></span><br><span class="line">        <span class="comment">#    break</span></span><br><span class="line">        sentimentslist.append(s.sentiments)</span><br><span class="line">        list_tmp = [i, s.sentiments]</span><br><span class="line">        list_all.append(list_tmp)</span><br><span class="line">        <span class="keyword">if</span> float(s.sentiments) &lt; <span class="number">0.5</span>:</span><br><span class="line">            list_neg.append(list_tmp)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            list_pos.append(list_tmp)</span><br><span class="line">file_do(list_all, <span class="number">0</span>)</span><br><span class="line">file_do(list_neg, <span class="number">1</span>)</span><br><span class="line">file_do(list_pos, <span class="number">2</span>)</span><br><span class="line">plt.hist(sentimentslist, bins = np.arange(<span class="number">0</span>, <span class="number">1</span>, <span class="number">0.01</span>), facecolor = <span class="string">'g'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Sentiments Probability'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Quantity'</span>)</span><br><span class="line">plt.title(<span class="string">'使用SnowNLP对《海上钢琴师》进行情感分析'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h4 id="分析的效果图"><a href="#分析的效果图" class="headerlink" title="分析的效果图"></a>分析的效果图</h4><p><del>最靠近1的那块数据应该是溢出了，但是我也不知道咋改</del><br><img src="../../images/snownlp.PNG" alt="avatar"></p>
<h4 id="测试SnowNLP的准确度"><a href="#测试SnowNLP的准确度" class="headerlink" title="测试SnowNLP的准确度"></a>测试SnowNLP的准确度</h4><p>猫眼的评分为5分制，设2.5分及以上的情感态度为positive，以下为negative。SnowNLP的评分为0-1，所以设0.5及以上的情感态度为positive，以下为negative。对1000组数据进行分析，895组数据的评分和SnowNLP分析的情感态度相同。因为存在一些人打分随意、打了正面分给了负面评价或反之的情况，所以这个准确度是可以接受的。</p>
<h3 id="使用CNN卷积神经网络"><a href="#使用CNN卷积神经网络" class="headerlink" title="使用CNN卷积神经网络"></a>使用CNN卷积神经网络</h3><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">stopwords=pd.read_csv(<span class="string">"../stopwords.txt"</span>,index_col=<span class="literal">False</span>,quoting=<span class="number">3</span></span><br><span class="line">                      ,sep=<span class="string">"\t"</span>,names=[<span class="string">'stopword'</span>], encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">stopwords=stopwords[<span class="string">'stopword'</span>].values</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_text</span><span class="params">(content_lines,sentences,category)</span>:</span> </span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> content_lines:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            segs=jieba.lcut(line)</span><br><span class="line">            segs = filter(<span class="keyword">lambda</span> x:len(x)&gt;<span class="number">1</span>, segs)</span><br><span class="line">            segs = filter(<span class="keyword">lambda</span> x:x <span class="keyword">not</span> <span class="keyword">in</span> stopwords, segs)</span><br><span class="line">            sentences.append((<span class="string">" "</span>.join(segs), category))</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            print(line)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成训练数据            </span></span><br><span class="line">sentences=[]</span><br><span class="line">preprocess_text(data_com_X_1.content.dropna().values.tolist() ,sentences ,<span class="string">'like'</span>)</span><br><span class="line">n=<span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> n &lt;<span class="number">20</span>:</span><br><span class="line">    preprocess_text(data_com_X_0.content.dropna().values.tolist() ,sentences ,<span class="string">'nlike'</span>)</span><br><span class="line">    n +=<span class="number">1</span></span><br><span class="line">random.shuffle(sentences)     </span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x,y=zip(*sentences)</span><br><span class="line">train_data,test_data,train_target,test_target=train_test_split(x, y, random_state=<span class="number">1234</span>)</span><br></pre></td></tr></table></figure>

<h4 id="构建两层CNN神经网络"><a href="#构建两层CNN神经网络" class="headerlink" title="构建两层CNN神经网络"></a>构建两层CNN神经网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">基于卷积神经网络的中文文本分类</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">learn = tf.contrib.learn</span><br><span class="line">FLAGS = <span class="literal">None</span></span><br><span class="line"><span class="comment"># 文档最长长度</span></span><br><span class="line">MAX_DOCUMENT_LENGTH = <span class="number">100</span></span><br><span class="line"><span class="comment"># 最小词频数</span></span><br><span class="line">MIN_WORD_FREQUENCE = <span class="number">2</span></span><br><span class="line"><span class="comment"># 词嵌入的维度</span></span><br><span class="line">EMBEDDING_SIZE = <span class="number">20</span></span><br><span class="line"><span class="comment"># filter个数</span></span><br><span class="line">N_FILTERS = <span class="number">10</span> <span class="comment"># 10个神经元</span></span><br><span class="line"><span class="comment"># 感知野大小</span></span><br><span class="line">WINDOW_SIZE = <span class="number">20</span></span><br><span class="line"><span class="comment">#filter的形状</span></span><br><span class="line">FILTER_SHAPE1 = [WINDOW_SIZE, EMBEDDING_SIZE]</span><br><span class="line">FILTER_SHAPE2 = [WINDOW_SIZE, N_FILTERS] </span><br><span class="line"><span class="comment"># 池化</span></span><br><span class="line">POOLING_WINDOW = <span class="number">4</span></span><br><span class="line">POOLING_STRIDE = <span class="number">2</span></span><br><span class="line">n_words = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cnn_model</span><span class="params">(features, target)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    2层的卷积神经网络，用于短文本分类</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 先把词转成词嵌入</span></span><br><span class="line">    <span class="comment"># 我们得到一个形状为[n_words, EMBEDDING_SIZE]的词表映射矩阵</span></span><br><span class="line">    <span class="comment"># 接着我们可以把一批文本映射成[batch_size, sequence_length,EMBEDDING_SIZE]的矩阵形式</span></span><br><span class="line">    target = tf.one_hot(target, <span class="number">15</span>, <span class="number">1</span>, <span class="number">0</span>) <span class="comment">#对词编码</span></span><br><span class="line">    word_vectors = tf.contrib.layers.embed_sequence(features</span><br><span class="line">                                                    ,vocab_size=n_words</span><br><span class="line">                                                    ,embed_dim=EMBEDDING_SIZE</span><br><span class="line">                                                    ,scope=<span class="string">'words'</span>)</span><br><span class="line">    word_vectors = tf.expand_dims(word_vectors, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'CNN_Layer1'</span>):</span><br><span class="line">        <span class="comment"># 添加卷积层做滤波</span></span><br><span class="line">        conv1 = tf.contrib.layers.convolution2d(word_vectors</span><br><span class="line">                                                ,N_FILTERS</span><br><span class="line">                                                ,FILTER_SHAPE1</span><br><span class="line">                                                ,padding=<span class="string">'VALID'</span>)</span><br><span class="line">        <span class="comment"># 添加RELU非线性</span></span><br><span class="line">        conv1 = tf.nn.relu(conv1) </span><br><span class="line">        <span class="comment"># 最大池化</span></span><br><span class="line">        pool1 = tf.nn.max_pool(conv1</span><br><span class="line">                               ,ksize=[<span class="number">1</span>, POOLING_WINDOW, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">                               ,strides=[<span class="number">1</span>, POOLING_STRIDE, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">                               ,padding=<span class="string">'SAME'</span>)</span><br><span class="line">        <span class="comment"># 对矩阵进行转置，以满足形状</span></span><br><span class="line">        pool1 = tf.transpose(pool1, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'CNN_Layer2'</span>):</span><br><span class="line">        <span class="comment"># 第2卷积层</span></span><br><span class="line">        conv2 = tf.contrib.layers.convolution2d(pool1</span><br><span class="line">                                                ,N_FILTERS</span><br><span class="line">                                                ,FILTER_SHAPE2</span><br><span class="line">                                                ,padding=<span class="string">'VALID'</span>) </span><br><span class="line">        <span class="comment"># 抽取特征</span></span><br><span class="line">        pool2 = tf.squeeze(tf.reduce_max(conv2, <span class="number">1</span>), squeeze_dims=[<span class="number">1</span>])</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 全连接层</span></span><br><span class="line">    logits = tf.contrib.layers.fully_connected(pool2, <span class="number">15</span>, activation_fn=<span class="literal">None</span>)</span><br><span class="line">    loss = tf.losses.softmax_cross_entropy(target, logits) </span><br><span class="line">    <span class="comment"># 优化器</span></span><br><span class="line">    train_op = tf.contrib.layers.optimize_loss(loss</span><br><span class="line">                                               ,tf.contrib.framework.get_global_step()</span><br><span class="line">                                               ,optimizer=<span class="string">'Adam'</span></span><br><span class="line">                                               ,learning_rate=<span class="number">0.01</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (&#123;</span><br><span class="line">            <span class="string">'class'</span>: tf.argmax(logits, <span class="number">1</span>),</span><br><span class="line">            <span class="string">'prob'</span>: tf.nn.softmax(logits)</span><br><span class="line">    &#125;, loss, train_op)</span><br></pre></td></tr></table></figure>

<h4 id="对词汇处理"><a href="#对词汇处理" class="headerlink" title="对词汇处理"></a>对词汇处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">global</span> n_words</span><br><span class="line"><span class="comment"># 处理词汇</span></span><br><span class="line">vocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH,min_frequency=MIN_WORD_FREQUENCE) </span><br><span class="line">x_train = np.array(list(vocab_processor.fit_transform(train_data)))</span><br><span class="line">x_test = np.array(list(vocab_processor.transform(test_data)))</span><br><span class="line">n_words=len(vocab_processor.vocabulary_) </span><br><span class="line">print(<span class="string">'Total words:%d'</span>%n_words)</span><br><span class="line"></span><br><span class="line">cate_dic=&#123;<span class="string">'like'</span>:<span class="number">1</span>,<span class="string">'nlike'</span>:<span class="number">0</span>&#125;</span><br><span class="line">y_train = pd.Series(train_target).apply(<span class="keyword">lambda</span> x:cate_dic[x] , train_target)</span><br><span class="line">y_test = pd.Series(test_target).apply(<span class="keyword">lambda</span> x:cate_dic[x] , test_target)</span><br><span class="line"><span class="comment"># Total words:370</span></span><br></pre></td></tr></table></figure>

<h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line">classifier=learn.SKCompat(learn.Estimator(model_fn=cnn_model))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练和预测</span></span><br><span class="line">classifier.fit(x_train,y_train,steps=<span class="number">1000</span>) </span><br><span class="line">y_predicted=classifier.predict(x_test)[<span class="string">'class'</span>] </span><br><span class="line">score=metrics.accuracy_score(y_test,y_predicted) </span><br><span class="line">print(<span class="string">'Accuracy:&#123;0:f&#125;'</span>.format(score))</span><br></pre></td></tr></table></figure>

<h4 id="效果图："><a href="#效果图：" class="headerlink" title="效果图："></a>效果图：</h4><p><img src="../../images/CNN.PNG" alt="avatar"></p>
<h4 id="测试CNN的准确度"><a href="#测试CNN的准确度" class="headerlink" title="测试CNN的准确度"></a>测试CNN的准确度</h4><p>测试标准同SnowNLP，只不过数据集变成了豆瓣的影评（可能质量会高一点），多次测试准确率大概在94%左右</p>
<h2 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h2><h3 id="生成词云"><a href="#生成词云" class="headerlink" title="生成词云"></a>生成词云</h3><p>使用wordcloud库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 导入扩展库</span></span><br><span class="line"><span class="keyword">import</span> re <span class="comment"># 正则表达式库</span></span><br><span class="line"><span class="keyword">import</span> collections <span class="comment"># 词频统计库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># numpy数据处理库</span></span><br><span class="line"><span class="keyword">import</span> jieba <span class="comment"># 结巴分词</span></span><br><span class="line"><span class="keyword">import</span> wordcloud <span class="comment"># 词云展示库</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image <span class="comment"># 图像处理库</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 图像展示库</span></span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取文件</span></span><br><span class="line">text = codecs.open(<span class="string">r'C:\Users\think\Desktop\情感分析\doc\answer.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>).read()</span><br><span class="line">print(text)</span><br><span class="line"><span class="comment">#fn = open(r'C:\Users\think\Desktop\情感分析\answer.txt') # 打开文件</span></span><br><span class="line"><span class="comment">#string_data = fn.read() # 读出整个文件</span></span><br><span class="line"><span class="comment">#fn.close() # 关闭文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本预处理</span></span><br><span class="line">pattern = re.compile(<span class="string">u'\t|\n|\.|-|:|;|\)|\(|\?|"'</span>) <span class="comment"># 定义正则表达式匹配模式</span></span><br><span class="line">string_data = re.sub(pattern, <span class="string">''</span>, text) <span class="comment"># 将符合模式的字符去除</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本分词</span></span><br><span class="line">seg_list_exact = jieba.cut(text, cut_all = <span class="literal">False</span>) <span class="comment"># 精确模式分词</span></span><br><span class="line">object_list = []</span><br><span class="line">remove_words = [<span class="string">u'的'</span>, <span class="string">u'，'</span>,<span class="string">u'和'</span>, <span class="string">u'是'</span>, <span class="string">u'随着'</span>, <span class="string">u'对于'</span>, <span class="string">u'对'</span>,<span class="string">u'等'</span>,<span class="string">u'能'</span>,<span class="string">u'都'</span>,<span class="string">u'。'</span>,<span class="string">u' '</span>,<span class="string">u'、'</span>,<span class="string">u'中'</span>,<span class="string">u'在'</span>,<span class="string">u'了'</span>,</span><br><span class="line">                <span class="string">u'通常'</span>,<span class="string">u'如果'</span>,<span class="string">u'我们'</span>,<span class="string">u'需要'</span>,<span class="string">u'他'</span>,<span class="string">u'我'</span>,<span class="string">u'看'</span>,<span class="string">u'电影'</span>,<span class="string">u'很'</span>,<span class="string">u'人'</span>,<span class="string">u'不'</span>,<span class="string">u'有'</span>,<span class="string">u'也'</span>,<span class="string">u'这'</span>,<span class="string">u'就'</span>,<span class="string">u'…'</span>,<span class="string">u'\r\n'</span>,<span class="string">u'还是'</span>,<span class="string">u'就'</span>,<span class="string">u'就是'</span>] <span class="comment"># 自定义去除词库</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> seg_list_exact: <span class="comment"># 循环读出每个分词</span></span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> remove_words: <span class="comment"># 如果不在去除词库中</span></span><br><span class="line">        object_list.append(word) <span class="comment"># 分词追加到列表</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 词频统计</span></span><br><span class="line">word_counts = collections.Counter(object_list) <span class="comment"># 对分词做词频统计</span></span><br><span class="line">print(word_counts)</span><br><span class="line">word_counts_top10 = word_counts.most_common(<span class="number">10</span>) <span class="comment"># 获取前10最高频的词</span></span><br><span class="line"><span class="keyword">print</span> (word_counts_top10) <span class="comment"># 输出检查</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 词频展示</span></span><br><span class="line"><span class="comment">#mask = np.array(Image.open(r'C:\Users\think\Desktop\情感分析\blackbeijing.jpg')) # 定义词频背景</span></span><br><span class="line">wc = wordcloud(</span><br><span class="line">    font_path=<span class="string">'C:/Windows/Fonts/simhei.ttf'</span>, <span class="comment"># 设置字体格式</span></span><br><span class="line">    width=<span class="number">2000</span>,height=<span class="number">1200</span>,</span><br><span class="line">    background_color=<span class="string">'white'</span>,  <span class="comment"># 设置背景颜色</span></span><br><span class="line">    <span class="comment">#mask=mask, # 设置背景图</span></span><br><span class="line">    max_words=<span class="number">200</span>, <span class="comment"># 最多显示词数</span></span><br><span class="line">    max_font_size=<span class="number">300</span>, <span class="comment"># 字体最大值</span></span><br><span class="line">    random_state=<span class="number">30</span>  <span class="comment"># 设置多少种随机状态，即多少种配色</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># wc = WordCloud(</span></span><br><span class="line"><span class="comment">#     #font_path='C:\Windows\Fonts\SIMYOU.TTF',  # 指定中文字体</span></span><br><span class="line"><span class="comment">#     background_color='white',  # 设置背景颜色</span></span><br><span class="line"><span class="comment">#     max_words=2000,  # 设置最大显示的字数</span></span><br><span class="line"><span class="comment">#     mask=bg_pic,  # 设置背景图片</span></span><br><span class="line"><span class="comment">#     max_font_size=200,  # 设置字体最大值</span></span><br><span class="line"><span class="comment">#     random_state=20  # 设置多少种随机状态，即多少种配色</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"><span class="comment"># wc.generate_from_frequencies(dict(word))  # 生成词云</span></span><br><span class="line">wc.generate_from_frequencies(word_counts) <span class="comment"># 从字典生成词云</span></span><br><span class="line"><span class="comment">#image_colors = wordcloud.ImageColorGenerator(mask) # 从背景图建立颜色方案</span></span><br><span class="line"><span class="comment">#wc.recolor(color_func=image_colors) # 将词云颜色设置为背景图方案</span></span><br><span class="line">plt.imshow(wc) <span class="comment"># 显示词云</span></span><br><span class="line">plt.axis(<span class="string">'off'</span>) <span class="comment"># 关闭坐标轴</span></span><br><span class="line">plt.show() <span class="comment"># 显示图像</span></span><br><span class="line">wc.to_file(<span class="string">r'C:\Users\think\Desktop\情感分析\src\result.jpg'</span>)</span><br></pre></td></tr></table></figure>

<h4 id="效果图：-1"><a href="#效果图：-1" class="headerlink" title="效果图："></a>效果图：</h4><p><img src="../../images/result.jpg" alt="avatar"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h5 id="数据处理和NLP真好玩！"><a href="#数据处理和NLP真好玩！" class="headerlink" title="数据处理和NLP真好玩！"></a>数据处理和NLP真好玩！</h5><h5 id="没想到我真正入门爬虫还是在大作业ddl的逼迫下！"><a href="#没想到我真正入门爬虫还是在大作业ddl的逼迫下！" class="headerlink" title="没想到我真正入门爬虫还是在大作业ddl的逼迫下！"></a>没想到我真正入门爬虫还是在大作业ddl的逼迫下！</h5><h5 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h5><p>1、数据分析的数据问题：仅通过短评文本来分类点评人对电影的喜好确实浅薄了些，因为太多的短评内容与打分之间的关联事很弱的，有着很大的随意性：“我反正给不了5星”-&gt;4星-&gt;like、“还是经典，有机会以后要看高清的”-&gt;0星-&gt;nlike…更何况，我们的训练数据label有着较为严重的样本不平衡的问题。所以，如果真的想较为全面且准确的预测点评人短评的喜好程度的话，就不仅需要更多且平衡label的训练数据，还需要考虑更多维度的信息。<br>2、自己的问题：没有版本管理的意识，python用的不熟（<del>还是excel方便</del>）</p>

    </div>

    
    
    

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2019/11/18/python-mp4tomp3/" rel="next" title="python：从视频中提取音频(使用MoviePy库 or ffmpeg+脚本批量下载)">
                  <i class="fa fa-chevron-left"></i> python：从视频中提取音频(使用MoviePy库 or ffmpeg+脚本批量下载)
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
            </div>
          </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据抓取"><span class="nav-number">1.</span> <span class="nav-text">数据抓取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#代码"><span class="nav-number">1.1.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#情感数据分析"><span class="nav-number">2.</span> <span class="nav-text">情感数据分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#使用SnowNLP库"><span class="nav-number">2.1.</span> <span class="nav-text">使用SnowNLP库</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#代码-1"><span class="nav-number">2.1.1.</span> <span class="nav-text">代码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#分析的效果图"><span class="nav-number">2.1.2.</span> <span class="nav-text">分析的效果图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#测试SnowNLP的准确度"><span class="nav-number">2.1.3.</span> <span class="nav-text">测试SnowNLP的准确度</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用CNN卷积神经网络"><span class="nav-number">2.2.</span> <span class="nav-text">使用CNN卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据预处理"><span class="nav-number">2.2.1.</span> <span class="nav-text">数据预处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#构建两层CNN神经网络"><span class="nav-number">2.2.2.</span> <span class="nav-text">构建两层CNN神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#对词汇处理"><span class="nav-number">2.2.3.</span> <span class="nav-text">对词汇处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#训练模型"><span class="nav-number">2.2.4.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#效果图："><span class="nav-number">2.2.5.</span> <span class="nav-text">效果图：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#测试CNN的准确度"><span class="nav-number">2.2.6.</span> <span class="nav-text">测试CNN的准确度</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据可视化"><span class="nav-number">3.</span> <span class="nav-text">数据可视化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#生成词云"><span class="nav-number">3.1.</span> <span class="nav-text">生成词云</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#效果图：-1"><span class="nav-number">3.1.1.</span> <span class="nav-text">效果图：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">4.</span> <span class="nav-text">总结</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#数据处理和NLP真好玩！"><span class="nav-number">4.0.0.1.</span> <span class="nav-text">数据处理和NLP真好玩！</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#没想到我真正入门爬虫还是在大作业ddl的逼迫下！"><span class="nav-number">4.0.0.2.</span> <span class="nav-text">没想到我真正入门爬虫还是在大作业ddl的逼迫下！</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#存在的问题"><span class="nav-number">4.0.0.3.</span> <span class="nav-text">存在的问题</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhang Yufei</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhang Yufei</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">22k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">20 分钟</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.5.0
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
















  

  

</body>
</html>
